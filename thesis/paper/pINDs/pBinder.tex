\section{Partial BINDER}
The \textit{BINDER} Algorithm \cite{papenbrock2015divide} is a IND discovery Algorithm which uses a divide and conquer technique to efficiently find unary and nary INDs. It was shown that \textit{BINDER} outperforms other exact, non-distributed state-of-the-art algorithms in both unary and nary settings \cite{dursch2019inclusion}. Due to its strong performance, this thesis offers a adapted version of \textit{BINDER} which can handle partial INDs, called \textit{pBINDER}.

\subsection{Existing Code}
The original \textit{BINDER} paper does not have a direct linkage to the source code used for the experiments. For the work conducted by DÃ¼rsch et al. on the comparison of multiple IND discovery algorithms\cite{dursch2019inclusion}, they published an implementation of \textit{BINDER} through GitHub\footnote{https://github.com/HPI-Information-Systems/inclusion-dependency-algorithms}. After talking to Thorsten Papenbrock, it turns out the source code found there is almost equal to the original code. The problem here lies within the poor structure of the code. Since a code base like this is neither readable nor well maintainable, the integration of partial constraints was a greater effort than initially expected. During the process of cleaning \textit{BINDER} I also bumped the needed dependencies to their most recent versions.

\subsection{Validator Adjustments}
The Validator of $BINDER$ handles the invalidation of IND candidates. Initially the algorithm assumes, that all candidates are valid. Whenever we find a conflicting value, meaning a value which is present in the depended side but not in the referenced side of a candidate, the validator removes the given candidate. To consider partial INDs we need to expand $BINDER$ such that the validator keeps track of the number of validations and only removes a candidate, if more validations than a given threshold have happened.

It is important to note that the partial $BINDER$ ($pBINDER$) implementation only supports a duplicate aware. The concept and efficiency of $BINDER$ lies in the divide and conquer approach. The attributes are split into a set of buckets using hash functions, such that the first bucket of each attribute maps the same subspace of values, e.g. there are $n$ buckets and we choses $hashCode \% n$ as the separation function. During the validation, the buckets are iterated in some order. At some point of the validation, we might already know, that the attribute is no longer included in any candidate. Loading the bucket now would be a waste of compute resources. There for we precisely skip these buckets. While this is great in a computational sense, $BINDER$ has no way of counting the number of distinct values, if not all buckets are loaded, especially if we would like to know the number of distinct values before the validation starts. Implementing duplicate unaware pIND discovery into $pBINDER$ would divert the algorithm too much from the original algorithm too a point where discussing the newly introduced complexity of pIND discovery would not make sense.



\begin{algorithm}
    \caption{Adjusted BINDER candidate pruning}\label{alg:BINDER_prune}
    \hspace*{\algorithmicindent} \textbf{Input:} value, valueGroup, pINDCandidates
    \begin{algorithmic}[1]
    \For{attribute in valueGroup}
        \State $occurrences = attribute.getOccurrences(value)$
        \For{candidate in pINDCandidates}
            \If{candidate.dependant $\not =$ attribute} 
                \State \textbf{continue}
            \EndIf
            \If{candidate.reference $\in$ valueGroup} 
                \State \textbf{continue}
            \EndIf
            \State candidate.violations += occurrences
            \If{candidate.violations $>$ candidate.maxViolations} 
                \State remove candidate
            \EndIf
        \EndFor
    \EndFor
    \end{algorithmic}
\end{algorithm}




% TODO test performance of rewrite and original
