\section{Partial n-ary candidate generation}
To discover n-ary pINDs, we need to generate candidates for the next layer using the already collected information of the current layer. An iterative process wherein progressively larger candidate sets are generated and verified follows the idea of an apriori-gen algorithm \cite{agrawal1994fast}. In related research we find proposals that only use the most recent layer to generate the candidate set for the following layer \cite{papenbrock2015divide}. We will propose a refined version, which significantly decreases the number of false candidates that need to be validated.

As proven (partial) INDs respect projection. For any pIND of size $n$, all subsets of that pIND must be also valid. This property has not been used to its full potential by past research. An adjusted version for improved candidate generation needs to follow the three rule below.

Given two not necessarily different relational schema $R_a$ and $R_b$ with valid (n-1)-ary attributes $X \subset R_a$ and $Y \subset R_b$ and valid u-nary attributes $A \in R_a$ and $B \in R_b$, a valid n-ary candidate is formed if
\begin{itemize}
    \item[1)] The column index of $A$ is greater than the column index of all $x_i \in X$.
    \item[2)] $B$ is not contained in $Y$.
    \item[3)] All (n-1)-ary pINDs $R_a[(X \setminus X_i)A] \subseteq_\rho R_b[(Y \setminus Y_i)B] : i \in (1, \dots, n-1)$ must be valid.
    \item[4)] If $R_a$ and $R_b$ are the same scheme, then $X$, $A$, $Y$ and $B$ need to be pairwise disjoint.
    
\end{itemize}

Originally, $B$ was also required to be non-empty, which makes sense when treating null as a subset. In that setting an all-null attribute will create a valid n-ary candidate with all (n-1)-ary pINDs and is therefore trivial. The proposed algorithm however offer different null interpretations under which all-empty columns might become relevant. This is why we will not require $B$ to be non-empty.

We will further apply another step to the candidate generation which improves the original method by pruning candidates that are not possible during the generation without the need of the actual relation instances. We have showed that transitivity requires all subsets of a candidate to be valid for that candidate to be valid (Section \ref{theo:pInd}). In the original generation, this is only applied in a weak setting by ensuring $r_1[X] \subseteq_\rho r_2[Y]$ and $r_1[A] \subseteq_\rho r_2[B]$. Assume $|X| = 3$, we know that if $r_1[X_1, A] \subseteq_\rho r_2[Y_1, B]$ or $r_1[X_2, A] \subseteq_\rho r_2[Y_2, B]$ are not present in the current layer, then $r_1[X_1, X_2, A] \subseteq_\rho r_2[Y_1, Y_2, B]$ can not be valid. Therefor the proposed expansion is to ensure the subsets of the generated candidates are also valid.

\begin{algorithm}[hbt!]
    \caption{Subset candidate check}\label{alg:canditate_gen}
    \KwInput{$rDep, nDep, uDep, rRef, nRef, uRef$}
    \KwOutput{Whether the next level candidate is possible}

    \For{$skip$ in $1, \dots, |nDep|$}{
        dependant = $rDep[(nDep \setminus nDep_{skip}), uDep]$ \\
        referred = $rRef[(nRef \setminus nRef_{skip}), uRef]$ \\
        \If{dependant $\not \subseteq_\rho$ referred}{
            \Return False}
}
    \Return True
\end{algorithm}

Using the subset check is only sensible when generating candidates for the third or higher layer. Unary candidates are trivially generated using all available columns and the second layer is a combination of two unary pINDs which directly implies that all subsets have to be valid.

\subsection{Serializing n-ary tuples.}
Our unary discovery is based on strategies which relie on hashing and value comparison (sorting). By default, a list of values (tuple) is a non-hashable type. We could define a custom function and implement comparison functions for tuples. It would require us to use different functions for unary and n-ary discovery, even if they practically do the same. Using a string representation of tuples instead, enables us to use the unary procedures for n-ary pIND discovery, creating a simpler algorithmic structure. \textit{BINDER} also uses this strategy, where the values contained in some multi-dimensional tuple are concatenated using "\#" as a separator. The tuple $("Marburg", "35037", "06421")$ gets transformed to the string $"Marburg\#35037\#06421"$. Concatenating values in such a way can produce incorrect results since there can be multiple tuples which create the same string. We search for for a bijective transformation $f$ which accepts a tuple and creates a string. The function used by \textit{BINDER} is not bijective since the tuples $(1\#1, 1)$ and $(1, 1\#1)$ will both produce the same string. While this proves to be highly unlikely in real datasets, we should still eliminate this point of failure.
Or proposal function will take a given $k$ dimensional tuple and first conduct a length encoding for the first $k-1$ values. These lengths are concatenated using ":", but any non-numerical character would work here. We mark the end of the length encoding using "|", again the precise choice of that character is not important. After the length encoding all values are concatenated without using a delimiter. The two tuples from before would be transformed to$("1\#1", "1") \rightarrow "3|1\#11"$ and $("1", "1\#1") \rightarrow "1|11\#1"$.
To prove that our proposal is indeed bijective, we need to make sure that every string that can be produced by our function originates form exactly one input. We will construct the inverse function $f^{-1}$ and show that the chain $f circlejoin {f^-1}$ is an identity function for all tuples. The inverse function will first search for the first occurrence of "|" which marks the end of the length encoding. It will then split the second part based on the length information to extract the inputs.
Given a $k$ dimensional tuple of strings $(s_1, \dots, s_k)$, $f$ produces the string $"len(s_1):len(s_2):\dots:len(s_{k-1})|s_1 s_2 \dots s_k"$. The inverse function $f^{-1}$ can now cut the second part at the correct indices re-producing the tuple $(s_1, \dots, s_k)$. This function is independent of the characters present in the strings, since we only care about the first occurrence of "|" which is guaranteed to be the end of the length encoding due to the explained serialization procedures. Algorithm \ref{alg:string_transformation} shows how the transformation and inverse transformation can be implemented.

\begin{algorithm}[hbt!]
    \caption{N-ary Attribute String Creation}\label{alg:string_transformation}
    \KwInput{\textit{attributeValues}: The list of string values}
    \KwOutput{A String representation of all attribute values}

    \If{attributeValues.length == 1}{
        \Return \textit{attributeValues}[1]
    }

    representation = "" \\
    \tcc{Create the length encoding}
    \For{index \textbf{in} 1, \dots, attributeValues.length - 1}{
        representation.append(attributeValues[index].length) \\
        representation.append(":")
    }
    representation.append("|") \\
    \tcc{Append the actual values}
    \For{value \textbf{in} attributeValues}{
        representation.append(value)
    }
    \Return representation
\end{algorithm}