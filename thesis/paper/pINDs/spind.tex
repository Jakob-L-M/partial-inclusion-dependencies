\section{SPIND Overview}

We will now discuss our proposed algorithm $SPIND$, which stands for \textbf{S}calable \textbf{P}artial \textbf{IN}clusion \textbf{D}ependency discovery. Further the german word $Spind$ is a special kind of closet and often multiple "Spinds" are placed next to each other. This is a metaphor to the algorithms procedure. Every input relation will be transformed to a "Spind" of sorted values with connected attributes (attribute combinations), which is surely bigger than a $BINDER$ bucket, but there are far fewer "Spinds" than $BINDER$ would create buckets.

\noindent \\ \textit{SPIND} solves multiple problems which no other algorithm solves. The architecture does not need to monitor main memory consumption or estimate the size of some objects. \textit{BINDER} uses heuristics to estimate if the content of a file can be loaded without overflowing the available main memory. In some rare cases this heuristic can fail leading to an execution failure. Another issue is the creation of many files. On datasets with a lot of pINDs algorithms like \textit{BINDER} or \textit{SPIDER} will create one file per attribute (combination) regardless of the dataset size. This can lead to millions of files being created, which total a few megabytes. On top of the problems that \textit{SPIND} solves, it also outperforms \textit{BINDER} and \textit{SPIDER} computationally by a factor of up to multiple magnitudes.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.49\textwidth]{figures/SPIND.pdf}
    \caption{Conceptual overview of \textit{SPIND}}
    \label{fig:spind}
\end{figure}

\subsection{Chunking the input relations}
Modern CPUs feature multiple cores capable of executing tasks concurrently. Previous studies have largely overlooked the potential of multi-threading. In contrast, $SPIND$ will leverage multi-threading extensively to maximize hardware utilization. Achieving this objective necessitates the identification of independent tasks capable of being processed in parallel.

\noindent \\ The execution starts by fetching some very basic information about the input relations. For each relational instance (input table) $SPIND$ will extract the header column (if present) and store the number of unary attributes (columns) each table has. Using a constant $CHUNK\_SIZE$ which is set by the user, we spilt each relation into somewhat equal parts. Each chunk will consist of at most $\lfloor \frac{CHUNK\_SIZE}{\# cloumns \: in \: relation} \rfloor$ rows. The complexity of processing a relation directly correlates to the number of total values in that relation. While this may be an oversimplification since there are many more factors, like the distribution of duplicate values, the raw size is easy to modify and a heuristic that can be applied without any specific dataset knowledge. Each of the resulting chunks is associated with exactly one relation and carries a subset of that relations rows.

\noindent \\ Chunking is done exactly once at the very start and is not repeated for n-ary layers. We reuse the same chunks in every layer of the n-ary pIND discovery.
% TODO Link section discussing chunking effectiveness.

\noindent \\ Since $SPIND$ almost always operates on the relation layer, a hash based partitioning, similar to $BINDER$, is not feasible. For the validation, we need to descend to the attribute layer and there we need to know which attributes (attribute combinations) share at least one value. A partitioning would therefore be required to split the dataset into $n$ groups $G_1, \dots, G_n$ such that for every tuple of values $t_i$ generate by some row, it holds that if $t_i \in G_j$ than 
\begin{itemize}
    \item[1)] $\forall \: t_k \in G \setminus G_j : t_i \cap t_k = \emptyset$
    \item[2)] and $t_i \cap t_k \not = \emptyset \: \forall t_k \in G_j$.
\end{itemize}
To find such groups we would need knowledge of all values in a relation. Even if we expand the groups row by row and merge when necessary, solving this grouping problem would create a lot of new complexity. This leads to the decision to use a plain horizontal splitting approach.

\noindent \\ At the end of the chunking phase we have a set of chucks for each relation, that all carry at most \textit{CHUNK\_SIZE} values. Larger relations have been split into more chunks while smaller relations are probably contained within a single chunk.

\subsection{Probabilistic Filtering}
Assume we would know the set $R$ of all values, which only occur in referred attributes. Such a set could be used to filter the values that would need to be sorted, merged and validated. How the filter is filled and where exactly it is used will be discussed in the sorting and validation sections. This section will explain the reason why such a filter can be used in the first way and which benefits it brings.

\noindent \\ Mathematically we require the set $R$ to hold the property $\forall \: ref \in candidates : ref \cap R = \emptyset$. Trivially, the empty set would be valid for $R$. Our goal is to find an $R$ which best approximates the union of all values present in referenced attributes (attribute combinations) without the values present in any dependant attributes.

\noindent \\ Since we may not be able to actually keep all of the values in main memory, we need a different approach of filtering the values. An important observation being, that we need to be certain, that all values which are present in at least one dependant attribute need to make it through the filter. If values which are only in dependant (or irrelevant) attributes make it through the filter, that creates more computational complexity, but will not create any incorrect results.

\noindent \\ These constraints can be full filled by building a probabilistic filter in which all values that appear in an dependant attribute at least once are stored. Such a filter can than be queried using a value to check if that value was added. If the value was added previously it will always return true. If the value was not added it may create a false positive at a rate of typically less than 5\%. There are multiple options for such filters.

\noindent \\ Research regarding probabilistic data structures focuses on two primary metrics govern research evaluation: the computational efficiency of insertion and query operations, and the required number of bits for storing individual values within the structure\cite{fan2014cuckoo}. Bloom filters are initialized with an anticipated number of elements and a target false positive rate. However, if additional values are incorporated beyond the initial expectation, Bloom filters may become oversaturated, leading to an approaching false positive rate of 1 \cite{tarkoma2011theory}. Bloom filters use about 44\% ($log_2(e)$) more space per key than the theoretical lower bound. There are probabilistic data structures which are closer to the theoretical lower bound, but these structures impose more complex strategies which are out of scope of this thesis \cite{fan2014cuckoo}.

\noindent \\ The size of the Bloom Filter is set to 100 million expected values with a false positive rate of 1\% by default. With these settings the filter will consume TODO bytes of main memory. If the available memory is very limited, the filter should be initialized with a smaller size or higher false positive rate. Table \ref{tab:filter} shows how many values are inserted into the bloom filter during u-nary pIND discovery for each dataset. In section \ref{sec:spind_val} we will discuss in detail how the values are inserted into the filter.

\begin{table}
    \begin{tabular}{c|c}
     dataset & values in filter\\ 
     \hline\hline
     data.gov & - \\ 
     \hline
     musicbrainz & - \\
    \end{tabular}
    \caption{The number of items that are inserted into the bloom filter for each dataset.} \label{tab:filter}
\end{table}

\subsubsection{Procedual Refining.} After each layer the set of values which are present in dependant attributes will be a subset or equal to the set of values of any of the preciding layers. If we construct the probabilistic filter after unary pINDs have been validated, we might can only masked out values based on the unary pINDs. At fours layer, the set of values that make up dependant attribute combinations might have shrunk significantly, but the filter could not accomondate for that. We could overcome this issue by reconstructing the filter during the validation of every layer. While this creates more computational effored it holds the potential of saving read and write operations during the sort and merge phases. To rebuild the filter in n-ary layer we use the inverse transformation (see Algorithm \ref{alg:string_transformation}) to extract the strings contained in the serialized value and compute the hashes. These steps are executed in the parallized section of the validation, while the insertion into the filter needs to be synchronized between threads.

\subsubsection{Filter evaluation.} In Figure TODO we find the execution times of all datasets (expect the most complex ones) using the bloom filter by constructing it once during u-nary discovery (\textit{Once}), using and refining the filter on every level (\textit{Refining}) and not using a probabilistic filter at all (\textit{None}). The y-Axis is relative to the longest execution time for each dataset and the displayed times are averaged over three executions for each mode. We find that using a filter that was build once or a refinded filter results in very similar execution times which are usually a bit faster than not using a filter. Since a refinded filter can save some read and write operations we will chose this version to put a smaller workload on the disk. Notice that the total execution time of \textit{Cars} is under one second for n-ary discovery, which results in the time it takes to construct the file to become quite significant. We will not give much weight to this observation since the dataset is so quick to solve anyway.


\subsection{Sorting Chunks}
% IDEA: spill based on occurrences to that point.
Our objective is to arrange each chunk in sorted order, a prerequisite for subsequent merging and validation procedures. Consistently, each chunk undergoes identical processing steps, executed concurrently.

We employ a \textit{CSVReader}\footnote{Provided by the open source library "Opencsv" \url{https://opencsv.sourceforge.net/}} for each chunk, specifying the relevant attribute combinations. Line by line processing of the chunk occurs, during which we update all associated attributes to their new value. In Section \ref{subsec:n-ary-stings} we discuss exactly how the values of n-ary attributes are assigned. For now we will take the existence of a proper string value for both unary and n-ary attributes as a given. To manage these values and their associations, we employ a hash based mapping structure where values are mapped to their respective attributes along with the frequency of occurrences, thus forming a nested map. To prevent main memory overflows, we constrain the size of this map using a constant denoted as $SORT\_SIZE$. If the map size reaches the specified threshold (which is divided by the number of threads) or when sorting is complete, the contained data is serialized to disk.

This storage process will first sort the key set of the outer map. Afterwards we iterate over the sorted entries and write two lines for each of them. The first line contains the string value, which is the entries key. Followed by the the attributes which formed the value combined with the occurrences. Thereby the second line persists the value of the entry. Figure \ref{fig:sorting} illustrates this workflow while sorting for unary pIND discovery. The Attributes are $1:$\textit{id}, $2:$\textit{postCode}, $3:$\textit{landline}. For every spilled file, we retain the path of the sorted (sub) chunk, ultimately returning a list of these paths.

A minor optimization conduct here is spilling values based on the sum of occurrences in the connected attributes. We know that in a single chunk, there are at most \textit{CHUNK\_SIZE} unique values. In n-ary layers there might be relations where the number candidates of than relation is greater than the number of columns. Therefor we can calculate that the maximal possible number of distinct values generated from a chunk is $N = \underbrace{\lfloor \frac{CHUNK\_SIZE}{\#cloumns} \rfloor}_{= \#rows} \cdot \#candidates$, since every row can produce at most one unique value for each candidate. We can now say that $\frac{N}{SORTED\_SIZE}$ is the largest possible minimum of occurrences over all present values. The proposed heuristic suggests to keep all values with more than $\frac{\#candidates \cdot CHUNK\_SIZE}{SORTED\_SIZE}$ occurrences which is greater than $\frac{N}{SORTED\_SIZE}$ in main memory and only spill those values which where not seen often. The idea being, that values which have occurred often will be more likely to occur often. Spilling these last can save complexity during merging since less candidate occurrences need to be deserialized.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{figures/Sorting.pdf}
    \caption{Simplified illustration of the sorting process.}
    \label{fig:sorting}
\end{figure}

\subsection{Merging (spilled) Chunks}
We now have a bunch of files which are all sorted by themselves. The next step is to merge all files which originated from the same relation. Again, this is computed in parallel. To avoid too many files being opened at the same time the constant \textit{MERGE\_SIZE} can be used to limit the number of files which are being sorted per thread. Due to multithreading the actual number of simultaneously opened files is $\#threads \cdot (MERGE\_SIZE + 1)$. The plus one originates since we always need to open the resulting output file of a merge as well. We attach a buffered reader to each file and use a priority queue to perform a k-way merge \ref{taniar2008high}. At the end of the merge phase, we have constructed a single sorted file for each relation.

\subsection{Validation}\label{sec:spind_val}
The validation process expects a sorted file per relation. The validation process works analog to the validation performed by \textit{SPIDER}. Given the sorted files we check which attributes (attribute combinations) share a value and move the readers forward bit by bit in a way that only those readers are updated which share the same smallest value. \textit{SPIDER} uses a priority queue which stores the head value of each reader. Such a data structure enables us to calculate the smallest of the head values in $log(M)$ time, $M$ being the number of attributes (attribute combinations). Assume the complexity of pruning the connected attributes to some value is $O(k)$. Validating all values $N$ would cost $O(k \cdot N \cdot log(M))$. Pruning is a synchronous task, meaning the active candidates need to be synchronized between every prune. We will therefor try to create the attribute groups in parallel and conduct pruning in the main thread.

\noindent \\ We introduce the last variable \textit{VALIDATION\_SIZE}. It is a fixed value which states how many values are loaded at the same time for each relation. Deserialization is computed in parallel for all readers and we calculate the smallest largest value for each relation queue after they have been refilled. This largest smallest queue value is calculate at most $\lceil\frac{N}{\textit{VALIDATION\_SIZE}}\rceil$ times if all $N$ values distributed over the different relations where distinct. The value groups are then also build in parallel using a stream architecture to efficiently include the synchronized pruning. The resulting complexity is  $O(k \cdot N + \lceil M \cdot \frac{N}{\textit{VALIDATION\_SIZE}}\rceil)$. Should $M$, the number of relations, be very large, the logarithm would yield a better theoretical complexity eventually. But since $M$ can be expected to be much smaller than the \textit{VALIDATION\_SIZE} and the proposed version operates mostly in parallel, we find that this concept of loading groups in parallel decreases the compute time by a factor of up to five and especially helps when processing large datasets.

