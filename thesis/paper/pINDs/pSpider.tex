\section{Parializing SPIDER}
The \textit{SPIDER} Algorithm \cite{bauckmann2006efficiently} was devised for the detection of unary Inclusion Dependencies (INDs), employing an all-column sort merge join technique. It operates in two main phases. Initially, it sorts each attribute and stores the sorted values in one file per attribute. Subsequently, it conducts a $k$-way merge while simultaneously validating the candidates. \\

\noindent To partially extend the \textit{SPIDER} algorithm, it becomes essential to monitor the frequency of each value alongside the total count of unique values. Bauckmann et al. have already proposed an adjusted algorithm capable of uncovering partial Inclusion Dependencies (pINDs). Their suggestion involves integrating a counter to monitor violations, invalidating candidates when the violation count exceeds a threshold. This approach is effective when duplicates are disregarded. However, if duplication distribution is of concern, the authors merely suggest retrieving such information from a database, as values are deduplicated during sorting. While this is valid, it remains unclear how occurrences are managed thereafter. Assuming the occurrences of all values surpass the capacity of main memory, querying the database for each value could becomes necessary, which presents a massive computational overhead. This thesis proposal will introduce a method capable of handling both a \textit{duplicateAware} and a \textit{duplicateUnaware} setting in a unified manner.

\subsection{Existing Code}
The original \textit{SPIDER} paper does not have a direct linkage to the source code used for the experiments. For the work conducted by DÃ¼rsch et al. on the comparison of multiple IND discovery algorithms\cite{dursch2019inclusion}, they published an implementation of \textit{SPIDER} through GitHub\footnote{\href{https://github.com/HPI-Information-Systems/inclusion-dependency-algorithms}{github.com/HPI-Information-Systems/inclusion-dependency-algorithms}}. This implementation will be referred to as the Metanome implementation. There is other research which proposes ideas to increase the performance of $SPIDER$, which mostly safes time by discussing the underlying data structures in detail and moving to a C++ based implementation \cite{smirnov2023fast}. They conclude that a speed-up of up to 5 times is possible through memory efficient value storing and a changed approach on value sorting. Instead of sorting during reading, as the Metanome implementation does it, they read the values to vectors and utilize sorting these vectors in parallel, if the main memory filled up or the end of the input is reached.

\subsection{Sorting Adjustments}
The original implementation uses a $SortedSet$ as its key structure during the attribute processing. Every value is put into the $SortedSet$ in the order of its occurrence. If at some point, the main memory of the executing machine surpasses a set threshold, the values a written to disk. This process is called spilling. We save a (sorted) subset of the data to free main memory and in the end merge the sorted chunks together. Choosing a $SortedSet$ holds the advantages, that the values are guaranteed to be sorted. Insertion, deletion or containment checks all have an $log(n)$ complexity, where $n$ represents the number of elements. Internally a self balancing red-black is used to guarantee the operation complexity under any values. A further advantage is, that a $SortedSet$ already deduplicates the values, which yield a unique set of keys to which we can add the counts during reading with very little overhead.

\noindent \newline While this seems to be the perfect structure, the experimental results have shown much room for improvement. It has proofed to be much more efficient to store the values in a hash based structure, like a HashMap, to deduplicate entries and keep track of occurrences and only sort the key set of the map lazily before it needs to be spilled to disk. A main downside to an implementation based on a $SortedSet$ is, that the rebalancing of the red-black tree can get fairly expensive. Inserting 10 million randomly shuffled numbers into a $SortedSet$ if 4-5 times slower than inserting the same amount into a HashMap and sorting the keys afterwards \footnote{\href{https://github.com/Jakob-L-M/partial-inclusion-dependencies/blob/main/experiments/src/DataStructures.java}{Source file of experiment available through GitHub.}}. These observations concluded in the choice of HashMaps that are sorted lazily when its actually needed instead of $TreeSet$.

\noindent \newline We have decided to structure the sorted files by writing the value in one line and carrying the number of occurrences in to directly following line. This format is easy to parse and does not required additional string operations like a two column encoding would.


\subsection{Validation Adjustments}
Before the validation begins, we are aware of the number of unique and total values of every attribute. This information is used to calculate the thresholds while respecting the duplication handling mode.

\noindent \newline The validation of \textit{pSPIDER} and \textit{SPIDER} are practically equal. The only difference begin, that \textit{pSPIDER} checks if the number of violations has surpassed a threshold while \textit{SPIDER} prunes immediately once the first violation occurred.