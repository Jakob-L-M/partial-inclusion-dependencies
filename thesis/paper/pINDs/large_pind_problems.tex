Even though $BINDER$ and $SPIDER$ show great computational performance, there are open problems in the algorithms designs which can quickly lead to an execution failing. It is known that $SPIDER$ struggles with datasets that have a larger number of total attributes. Since the algorithm relies on opening one sorted file per attribute during the validation process, the operating systems needs to allow opening $|attributes|$ files at the same time. Linux systems have a default limit of 1024 open files per process, macOS limits a process to 256. While these limits can be increased drastically, a user might not know how to do that or lacks the privileges required. $SPIDER$ was only designed for u-nary IND discovery, where the attribute space is usually rather small compared to higher layers. The ideas behind $SPIDER$ could certainly be expanded to n-ary IND discovery, but the open files required may grow exponentially, causing operating errors to be very likely eventually. \\

\noindent $BINDER$ solves these issues by partitioning the attributes into buckets which should (heuristically) fit into main memory. To accomplish this a minimum of 10 files are written per attribute (combination). In a n-ary setting the number of files can quickly reach over 1mil (e.g. ACNH dataset). While the number of opened files is constant (one at a time) that total number of files pose a huge I/O overhead. Further the heuristically main memory calculations become more and more imprecise the more attributes (combinations) are present. While current research suggest, that $BINDER$ delivers very strong performance in n-ary IND detection, we will show that a new approach can outperform $BINDER$ by multiple magnitudes, especially when datasets yield a lot and complex (p)INDs.