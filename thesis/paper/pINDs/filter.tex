\subsection{Probabilistic Filtering}\label{subsec:prob_filter}

Suppose that we have the set $I$ containing all values that never appear in a dependent attribute. This set can be utilized to exclude values that are irrelevant for pIND discovery. There is no need to sort, merge, or validate these values. This section discusses why such a filter is useful and the advantages it offers.

Mathematically, we require the set $I$ to hold the property
$$u \cap I = \emptyset \: \forall \: u \in r_i[X],$$
where $r_i[X]$ is the left-hand side of a valid pIND.
Trivially, the empty set would be valid for $I$. Our goal is to find the largest set $I$ which can be build without significant computational overhead. Given the limitation of storing all values in main memory, an alternative method for filtering values is necessary. It is crucial to ensure that any value appearing in at least one dependent attribute passes through the filter. Although values found solely in referred or irrelevant attributes may increase computational complexity if they pass through the filter, they will not lead to incorrect results.

These constraints can be satisfied by constructing a probabilistic filter that stores all values appearing in a dependent attribute at least once. This filter can then be queried with a value to verify if it was added. If the value was previously added, the filter will always return true. If the value was not added, it can produce a false positive with a probability of typically less than 5\%. Studies on probabilistic data structures concentrate on two primary metrics that guide research assessment: the computational efficiency of insertion and query operations, and the required number of bits for storing individual values within the structure \cite{fan2014cuckoo}. Bloom filters are initialized with an anticipated number of elements and a target false positive rate. However, if additional values are incorporated beyond the initial expectation, Bloom filters may become oversaturated, leading to an approaching false positive rate of one \cite{tarkoma2011theory}. Bloom filters use about 44\% more (precisely $\log_2(e)$) space per key than the theoretical lower bound. There are probabilistic data structures which are closer to the theoretical lower bound \cite{fan2014cuckoo}, but these structures impose more complex strategies which is why we decided to choose a Bloom filter for \textit{SPIND}. The size of the Bloom Filter is set to 100 million expected values with a false positive rate of 1\% by default. With these settings, the filter consumes about 400 MB of main memory. If the memory available is highly restricted, the filter should be set up with a reduced size or an increased false positive rate. Refer to Section \ref{sec:spind_val} for details on the insertion of values into the filter.

\subsubsection{Procedural Refining.} After each layer, the values in the dependent attributes will be a subset or equal to those in preceding layers. Constructing the bloom filter after validating unary pINDs allows masking values based on unary pINDs. By the fourth layer, the dependent attribute values may have shrunk significantly, but the filter could not accommodate this. Reconstructing the filter at each layer's validation increases computational effort, but can save read and write operations during following sort and merge phases. To rebuild the filter in nary layers, we use our inverse string transformation (see Section \ref{subsec:nary-strings}) to extract strings from the serialized value and compute hashes. These steps are parallelized, while filter insertion must be synchronized between threads.
