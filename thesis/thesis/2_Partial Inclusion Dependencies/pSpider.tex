\section{Parializing SPIDER}
The \textit{SPIDER} Algorithm \cite{bauckmann2006efficiently} ...

\subsection{Existing Code}

The original \textit{SPIDER} paper does not have a direct linkage to the source code used for the experiments. For the work conducted by DÃ¼rsch et al. on the comparison of multiple IND discovery algorithms\cite{dursch2019inclusion}, they published an implementation of \textit{SPIDER} through GitHub\footnote{https://github.com/HPI-Information-Systems/inclusion-dependency-algorithms}. This implementation will be referred to as the Metanome implementation. There is other research which proposes ideas to increase the performance of $SPIDER$, which mostly safes time by discussing the underlying data structures in detail and moving to a C++ based implementation \cite{smirnov2023fast}. They conclude that a speed-up of up to 5 times is possible through memory efficient value storing and a changed approach on value sorting. Instead of sorting during reading, as the Metanome implementation does it, they read the values to vectors and utilize sorting these vectors in parallel, if the main memory filled up or the end of the input is reached.

\subsection{Validator Adjustments}


\subsection{Data Structures}
The original implementation uses a $SortedSet$ as its key structure during the attribute processing. Every value is put into the $SortedSet$ in the order of its occurrence. If at some point, the main memory of the executing machine surpasses a set threshold, the values a written to disk. This process is called spilling. We save a (sorted) subset of the data to free main memory and in the end merge the sorted chunks together. Choosing a $SortedSet$ holds the advantages, that the values are guaranteed to be sorted. Insertion, deletion or containment checks all have an $log(n)$ complexity, where $n$ represents the number of elements. Internally a self balancing red-black is used to guarantee the operation complexity under any values. A further advantage is, that a $SortedSet$ already deduplicates the values, which yield a unique set of keys to which we can add the counts during reading with very little overhead.
While this seems to be the perfect structure, the experimental results have shown much room for improvement. It has proofed to be much more efficient to store the values in a hash based structure, like a HashMap, to deduplicate entries and keep track of occurrences and only sort the key set of the map lazily before it needs to be spilled to disk. A main downside to an implementation based on a $SortedSet$ is, that the rebalancing of the red-black tree can get fairly expensive. Inserting 10 million randomly shuffled numbers into a $SortedSet$ if 4-5 times slower than inserting the same amount into a HashMap and sorting the keys afterwards \footnote{\href{https://github.com/Jakob-L-M/partial-inclusion-dependencies/blob/main/experiments/src/DataStructures.java}{Source file of experiment available through GitHub.}}. This factor gets even larger if we include the occurrences, since every occurrence update and the final read out adds additional operations of $O(log(n))$ compared to the $O(1)$ operations offered by the HashMap. \\

\noindent
Occasionally checking for memory overflows is neither robust nor resource efficient. The Java MemoryMXBean classifies used memory as all objects with 