% This document should discuss passed approaches focusing on pros and cons
\chapter{Related Work}\label{sec:rel_work}

Inclusion dependencies (INDs) are a highly influential concept in both database research and practice, with a wide range of contributions and applications. In this section, we will focus on the key achievements related to the implication problem of INDs. We will go over different algorithms and discuss their unique features.

In 1981 INDs started as a general notation of referential integrity, which was already a well established concept back then \cite{date1981referential}. Casanova et al. presented a paper on the inference rules of INDs \cite{casanova1982inclusion}. Three axioms where introduced: \textit{reflexivity}, \textit{transitivity} and \textit{projection and permutation}. We discuss if these rules can be applied to pINDs in Section \ref{subsec:pIND_props}. The paper further proofed that the discovery of INDs is PSPACE-complete if there is no limit on the size of inclusions. Publications typically fall into three: groups of algorithms, foreign key discovery algorithms, unary IND discovery, and n-nary IND discovery \cite{papenbrock2017data}.
Bell and Brockhausen \cite{bell1995discovery} proposed a graph-based approach in 1995 to represent the relationships between attributes, allowing a more efficient exploration of the search space. The algorithm is initiated with a directed graph, wherein all possible edges, which could not be pruned by statistical measures, are included. A directed edge in the graph represents an inclusion dependency, which is read as the edge from $A_i$ to $A_j$ ($A_i \rightarrow A_j$) representing the IND $A_i \subseteq A_j$. It then proceeds to remove those edges that failed the IND check. To determine the validity of an edge, the algorithm checks for transitivity, which enables it to answer whether a dependency could exist between two variables based on their relationships with a third variable, which was tested previously. If a dependency is impossible, the algorithm skips the test and directly removes the given edge, thereby reducing the overall computational cost. The approach presented by Bell and Brockhausen for unary IND discovery has both reusable aspects and downsides. The algorithm's candidate generation technique, which uses data statistics such as data types and min-max values, can be reused in other discovery algorithms. This preprocessing step reduces the number of candidates that need to be validated and further reduce the storage overhead needed to store candidates. However, the validation technique used in the algorithm, which relies on SQL join-statements and requires accessing the data on disk, is infeasible for larger candidate sets. This limits the scalability of the approach and makes it less practical for large-scale data sets. Additionally, the need to store the data in a database and access it on disk for validation can add to the computational cost and time required for the discovery process. In Section \ref{subsec:pIND_props}, we will find that pINDs do not satisfy transitivity, which requires a rethinking of algorithmic strategies.

The \textit{SPIDER} algorithm is a disk-backed, all-column sort-merge join with early termination used for the discovery of inclusion dependencies \cite{bauckmann2006efficiently}. It sorts the values of each attribute, removes duplicate values, and writes the results to disk in the first phase. In the second phase, it performs the actual inclusion dependency discovery by using a pointer for each file and validating all candidates at the same time. A major advantage is, that in this setting every value only needs to be read a single time from disk, which greatly reduces the I/O bottleneck. The Spider algorithm has been the subject of experimental evaluation and is considered one of the efficient techniques for unary IND discovery. Still, there are drawback if the data set is too big to be sorted in main memory or if the number of simultaneously open files allowed by the OS system is reached \cite{papenbrock2015divide}.

In 2009 Bauckmann et al. also proposed a partialized version of \textit{SPIDER} in a section of the same paper. The authors found that there where surprisingly many partial inclusions (under a 5\% threshold) in their test data sets. To find partial inclusion dependencies, they first count how many distinct violations are present and in a second step consider the amount of duplicates for not included values. This means their algorithm does not immediately stop once a single validation has been found but only after an added counter surpasses a given threshold. The paper is not particular clear on how the number of duplicates is stored/retrieved and additionally does not analyze the computational effect of these changes and with the original source code being lost, the strategies can only be verified using a best guess approach.

The \textit{BINDER} (Bucketized INclusion DEpendency discoveRy) algorithm was introduced in 2015 and offers significant advancements in the field of IND discovery \cite{papenbrock2015divide}. \textit{BINDER} employs a divide-and-conquer approach to efficiently detect both unary and nary INDs in relational databases. By partitioning data into buckets and utilizing metadata, \textit{BINDER} minimizes the number of expensive data comparisons required and reduces I/O operations. This approach allows \textit{BINDER} to scale well on large datasets since large portions of the dataset can potentially be disregarded in the conquer phase once a small portion has been processed. The algorithm's efficiency makes it particularly well-suited for handling large datasets and complex schemas, addressing limitations of previous IND discovery methods \cite{dursch2019inclusion}.

The \textit{SAWFISH} algorithm \cite{kaminsky2023discovering}, published in 2023, is designed for identifying similarity inclusion dependencies (sINDs) within data sets, introducing a novel perspective on inclusion dependencies (INDs). While traditional INDs assume error-free data, \textit{SAWFISH} incorporates a similarity measure to accommodate minor errors like typos. Given a similarity threshold $\omega$ a sIND is valid if and only if for all tuples in the left hand side there exists a tuple in the right hand side which has at least a similarity of $\omega$ under a set similarity measure. The authors used the edit distance as well as the normalized edit distance. Through preprocessing, metadata generation, and a sliding-window approach, \textit{SAWFISH} successfully identifies and validates sIND candidates using an inverted index, providing a valuable tool for database applications despite dirty data challenges.

The term \textit{partial inclusion dependency} also appears in another research paper with a different definition \cite{kohler2015inclusion}. The authors use the terms partial and simple IND to describe how \textit{NULL} values are handled during IND discovery. We also discuss \textit{NULL} handling in Chapter\ref{sec:null_handling} using a different wording.

Lastly, we have not identified an approach that concentrates solely on partial inclusion dependencies. This serves as the primary motivation for our work, along with re-evaluating algorithmic strategies using modern hardware (SSDs).

