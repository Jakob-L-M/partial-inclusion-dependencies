To ensure that the content of this thesis is readable by both experts and a general audience we need to formulate notations and definitions. These will reappear multiple times within the thesis and they are needed to formulate precise observations and draw conclusions. This sections sticks to the notation introduced by De Marchi et al. \cite{marchi2009unary}. \\

\noindent A relational instance $r$ of a relational schemata $R$ carries tuples of values, typically donated as $u$ or $v$. Using an attribute list taken from $R$, typically denoted as $X$ or $Y$, we can perform a projection on $R$, thereby selecting a subset of attributes. We notate it by $R[X]$. The same is possible for tuples $u$. Writing $u[X]$ references the selection of values in the tuple. \\

\begin{restatable}[Schemata and Attributes]{example}{schema}\label{exmp:schema}
% Rewrite
A relational instance can be thought of as a table where the schema is the structure of that table. An attribute can be imagined as a column in some table. If you think about picking multiple columns of some table, that would be an attribute list ($X$). Now every row has more than one value (a tuple of values, $u$), but every row has the exact same number of values (all tuple cardinalities are equal).
\end{restatable}

\noindent Inclusion Dependencies (INDs) represent a fundamental concept, denoting formal relationships between attributes in a database schema. An IND specifies that the values within one set of attributes are inherently included within the values of another set of attributes.

\begin{definition}[Inclusion Dependencies]\label{def:inds}
    Given two relational instances $r_i$ from $R_i$ and $r_j$ from $R_j$. An IND is defined as $R_i[X] \subseteq R_j[Y] \iff \forall \: u \in r_i[X], \exists \: v \in r_j[Y] \text{ such that } u[X] = v[Y]$. This condition can only hold if the cardinality of $X$ is equal to the cardinality of $Y$. We further call the left hand side (here $R_i[X]$) the dependent attribute(s) and the right hand side the referenced attribute(s).
\end{definition}

\begin{restatable}[Inclusion Dependencies]{example}{IND}\label{exmp:IND}
    The Tables \ref{tab:relExamp} show two small examples of relational data. The left table ($r_1$) has the attributes \textit{ID}, \textit{Name}, \textit{State} and \textit{Age}. The right table ($r_2$) has the attributes \textit{Name}, \textit{State} and \textit{Color}. We find the INDs $r_1[Name] \subseteq r_2[Name]$ and $r_2[State] \subseteq r_2[State]$. If $r_1$ had more entries and the \textit{ID} would just keep counting up, $r_1[Age] \subseteq r_1[ID]$ would be come valid eventually. There are no INDs where the attribute cardinality is greater than one.
\end{restatable}

\begin{table}[]
    \centering
    \begin{tabular}{c|c|c|c}
        ID & Name & State & Age \\
        \hline
        1 & Robin & HE & 22 \\
        2 & Hannah & BW & 24 \\
        3 & Christian & HE & 36 \\
        4 & Jakob & BW & 24 \\
        5 & Luka & BE & 23 \\
        6 & Mareike & BE & 22 \\
        7 & Leon & BE & 27 \\
    \end{tabular}
    \begin{tabular}{c|c|c}
        Name & State & Color \\
        \hline
        Jakob & BW & Green \\
        Mareike & BE & Purple \\
        Christian & HE & Red \\
        Leon & SH & Orange \\
    \end{tabular}
    \caption{Two example relational instances $r_1$ (left) and $r_2$ (right).}
    \label{tab:relExamp}
\end{table}

\noindent The complexity of discovering inclusion dependencies forms one of the hardest problems in computer science. More precisely, the discovery of all inclusion dependencies is W[3]-hard \cite{blasius2017parameterized}. This makes IND discovery one of the hardest problems in computer science. The number of possible candidates for each attribute size can be calculated. Notice that the formula below assumes that all IND of layers before where valid. In natural language, we search for the number of attribute combinations where each attribute is present at most once, allowing all permutations.

\begin{definition}[Candidate Space]\label{def:candidates}
    Let $\alpha$ be the fixed integer size of all possible $X_i$. Let $m$ be the number of attributes. Let $k$ be the number of possible candidates ($X_i \cap X_j = \emptyset$ where $i \not = j$ given $\alpha$).
    \[
        k = \underbrace{\binom{m}{\alpha}\cdot \alpha ! }_{\text{Left hand side combinations}} \cdot \underbrace{\binom{m-\alpha}{\alpha}\cdot \alpha!}_\text{Right hand side combinations}.
    \]
    This formula holds if $\alpha \leq \lfloor \frac{n}{2} \rfloor$ else $k$ is $0$.
\end{definition}

\noindent In a multi schema setting this calculation becomes more difficult. We now need to consider which schema can form which inter schema candidates while allowing intra schema candidates.
\begin{definition}[Candidate Space (Multi-Schema)]\label{def:candidates-MS}
    We will first define a function $q_r(\alpha, \tau)$, which computes the candidates a single relation ($r$) can produce, given $\alpha$, the size of combinations and $\tau$ an indicator, whether or not the schema was already used for the other side.

    \begin{align*}
        q_r(\alpha, \tau) = \begin{cases}
            0, & \text{if } \alpha > |r| \bigvee (\alpha > \lfloor\frac{|r|}{2}\rfloor \bigwedge \tau)\\
            \binom{|r|}{\alpha}\cdot \alpha!, & \text{if not } \tau \\
            \binom{|r| - \alpha}{\alpha}\cdot \alpha!, & \text{if } \tau
        \end{cases}
    \end{align*}
    Using this helper function we can calculate the possible candidates $k$ for a fixed $\alpha$ over schema $r_1, \dots, r_n$ by computing:
    $$
        k = \sum\limits_{i = 1}^n q_{r_i}(\alpha, FALSE) \cdot \sum\limits_{i = 1}^n q_{r_i}(\alpha, TRUE).
    $$
\end{definition}

\begin{definition}[Partial Inclusion Dependencies]\label{def:pinds}
    A partial inclusion dependency (pIND) is written as $r_1[X] \subseteq_{\rho} r_2[Y]$ where $\rho \in (0, 1]$ and the reaming notation is analog to Definition \ref{def:inds}. Here, the $\rho$ interval is not including $0$ since this would mean everything is a pIND of everything else, which is a trivial case. Further this notation refers to lists of tuples and takes the cardinality of duplicates into consideration. For the pIND $r_1[X] \subseteq_{\rho} r_2[Y]$ to be valid
    $$
        \frac{|r_1[X] \cap r_2[Y]|}
            {|r_1[X]|} \geq \rho
    $$
    needs to be true.
\end{definition}

\noindent 
This definition uses the $\cap$ operator, which is usually used in set theory and defined for sets. 
The definitions adaptation of the operator is $r_1[X] \cap r_2[Y] = u \in r_1[X] \: | \: \exists \; v \in r_2[Y] : u = v$.
In natural language the operator conducts the operation \textit{"remove all elements from the dependent attribute, that are not present at least once in the referenced attribute"}. Therefore the referenced attribute could also be a set in a sense that the duplication distribution only matters for the dependent attribute.\\

\noindent The proposed algorithms support two versions of duplicate handling. These versions where already proposed in \cite{bauckTODO} and can be defined as $duplicateAware$ and $duplicateUnaware$. For the $duplicateAware$ version, the maximum number of violations that can occur for $r_1[X] \subseteq_{\rho} r_2[Y]$ to be valid is equal to $\lfloor (1-\rho) \cdot |r_1[X]| \rfloor$. We examine if less than $\rho$ percent of entries would need to be changed. In a $duplicateUnaware$ setting we care about changed value, while not regarding their number of occurrences. We ask the question, do less than $\rho$ percent of the unique values need to be modified, such that the IND $r_1[X] \subseteq r_2[Y]$ would be valid. In this setting, the maximum number of violations is equal to $\lfloor(1-\rho) \cdot |\{r_1[X]\}| \rfloor$. Notice the set notation around the attribute, where we calculate the number of distinct values. \\

\begin{restatable}[Partial Inclusion Dependency Properties]{theorem}{pInd}\label{theo:pInd}
    Like inclusion dependencies, partial inclusion dependencies also fulfill the reflexive rule. For any $\rho \in (0, 1]$ the partial inclusion dependency $r_i[X_j] \subseteq_{\rho} r_i[X_j]$ is valid.
    \begin{align*}
        \frac{|r_i[X_j] \cap r_i[X_j]|}
            {|r_i[X_j]|} & \geq \rho \\
        \frac{|r_i[X_j]|}
            {|r_i[X_j]|} & \geq \rho \\
            1 & \geq \rho
     \end{align*}
     Since $\rho$ is upper bounded by $1$ the last statement will always be true. \\

     \noindent Contrary to INDs, pINDs do not generally respect transitivity if $\rho < 1$. We will proof this claim by contradiction. Assume $r_1[X] = [1, 2, ..., 100], r_2[Y] = [2, ..., 1000], r_3[Z] = [10, 11, ..., 1000],$. If transitivity would hold for any $\rho$, then we should find that for $\rho \in (0, 1]$ where $r_1[X] \subseteq_\rho r_2[Y], r_2[Y] \subseteq_\rho  r_3[Z]$ are valid, $ r_1[X] \subseteq_\rho  r_3[Z]$ also needs to be valid. For the given example, if $\rho = 0.95$, we find a contradiction. \\

     \noindent Lastly, INDs and also pINDs respect projection. We will now outline a proof for this claim. Consider the attributes $r_1[X], r_2[Y], r_3[Z], r_4[W]$ where $r_1[X]$ and $r_2[Y]$ are in the same relation and $r_3[Z]$ and $r_4[W]$ are in the same relation. Assume $r_1[X], r_2[Y] \subseteq_\rho r_3[Z], r_4[W]$ is valid for some $\rho \in (0, 1]$. If projection holds, this implies that $r_1[X] \subseteq_\rho r_3[Z]$ and $r_2[Y] \subseteq_\rho r_4[W]$ have to be valid as well. If we now only consider the portion (with reduced size $\rho\%$) which satisfies $r_1[X], r_2[Y] \subseteq_1 r_3[Z], r_4[W]$ we can use the known properties for INDs and conclude that for at least $\rho\%$ $r_1[X] \subseteq_1 r_3[Z]$ and $r_2[Y] \subseteq_1 r_4[W]$ has to be valid. This also directly implies that $r_1[X] \subseteq_\rho r_3[Z]$ and $r_2[Y] \subseteq_\rho r_4[W]$ will be true if the remaining $(1-\rho)\%$ values are added again. This property is very important for search space pruning, which is the single most important task for (p)IND discovery \cite{liu2010discover}.
\end{restatable}

\begin{restatable}[Partial Inclusion Dependencies]{example}{pInd}\label{exmp:pInd}
    Let us consider the two attributes $r_1[X] = [1, 2, 3, 4]$ and $r_2[Y] = [1, 1, 2, 3]$. First we can conclude, that $r_2[Y] \subseteq r_1[X]$ is an inclusion dependency, since all values present in $r_2[Y]$ also occur in $r_1[X]$. This also directly causes $r_2[Y] \subseteq_{1.0} r_1[X]$ to be a valid pIND. We will now calculate the maximal partial thresholds. Like inclusion dependencies, partial inclusion dependencies are not symmetrical, this requires us to perform two calculations. We already discovered $r_2[Y] \subseteq_{1.0} r_1[X]$, which implies the maximal threshold is $1$. Let us use the proposed formula for the other direction.
    \begin{align*}
        \frac{|r_1[X] \cap r_2[Y]|}
            {|r_1[X]|} & \geq \rho \\
        \frac{|[1, 2, 3, 4] \cap [1, 2, 3]|}
            {|[1, 2, 3, 4]|} & \geq \rho \\ 
        \frac{|[1, 2, 3]|}
            {|[1, 2, 3, 4]|} & \geq \rho \\
        \frac{3}{4} & \geq \rho. \\ 
    \end{align*}
    We have found that when the threshold $\rho$ is less or equal to $0.75$, $r_1[X] \subseteq_{\rho} r_2[Y]$ is valid.
\end{restatable}

\noindent The complexity of discovering partial inclusion dependencies is inherited form the base problem. Since the complexity is calculated under a worst case assumption, it does not change when switching into the partial setting. The thesis will discuss in detail how the time complexity behaves under varying thresholds.

