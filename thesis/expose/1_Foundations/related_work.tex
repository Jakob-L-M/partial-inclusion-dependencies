% This document should discuss passed approaches focusing on pros and cons

\section{Related Work}\label{sec:rel_work}

Inclusion dependencies (INDs) are a highly influential concept in both database research and practice, with a wide range of contributions and applications. The introduction already provided some insight into their diverse application areas of INDs. In this section, we will focus on the key achievements related to the implication problem of INDs. We will go over different algorithms and discuss their unique features. \\

In 1981 INDs started as a general notation of referential integrity, which was already a well established concept back then \cite{date1981referential}. Casanova et al. presented a paper on the inference rules of INDs\cite{casanova1982inclusion}. Three axioms where introduced: \textit{reflexivity}, \textit{transitivity} and \textit{projection and permutation}. The application of these rules to partial INDs will later be discussed in detail (Section \ref{theo:pInd}). The paper further proofed that the discovery of INDs is PSPACE-complete if there is no limit on the size of inclusions. Publications typically fall into three groups of algorithms, foreign key discovery algorithms, unary IND discovery and n-nary IND discovery \cite{papenbrock2017data}. \\

In 1995 Bell and Brockhausen \cite{bell1995discovery} propose a graph-based approach to represent the relationships between attributes, allowing for a more efficient exploration of the search space. The algorithm is initiated with a directed graph, wherein all possible edges, which could not be pruned by statistical measures, are included. A directed edge in the graph represents an inclusion dependency, which is read as the edge from $A_i$ to $A_j$ ($A_i \rightarrow A_j$) represents the IND $A_i \subseteq A_j$. It then proceeds to remove those edges that failed the IND check. To determine the validity of an edge, the algorithm checks for transitivity, which enables it to answer whether a dependency could exist between two variables based on their relationships with a third variable, which was tested previously. If it is ascertained that a dependency is impossible, the algorithm skips the test and directly removes the given edge, thereby reducing the overall computational cost. The approach presented by Bell and Brockhausen for unary IND discovery has both reusable aspects and downsides. The algorithm's candidate generation technique, which uses data statistics such as data types and min-max values, can be reused in other discovery algorithms. This preprocessing step reduces the number of candidates that need to be validated and further reduce the storage overhead needed to store candidates. However, the validation technique used in the algorithm, which relies on SQL join-statements and requires accessing the data on disk, is infeasible for larger candidate sets. This limits the scalability of the approach and makes it less practical for large-scale data sets. Additionally, the need to store the data in a database and access it on disk for validation can add to the computational cost and time required for the discovery process.\\

The \textit{SPIDER} algorithm is a disk-backed, all-column sort-merge join with early termination used for the discovery of inclusion dependencies \cite{bauckmann2006efficiently}. It sorts the values of each attribute, removes duplicate values, and writes the results to disk in the first phase. In the second phase, it performs the actual inclusion dependency discovery by using a pointer for each file and validating all candidates at the same time. A major advantage is, that in this setting every value only needs to be read a single time from disk, which greatly reduces the I/O bottleneck. The Spider algorithm has been the subject of experimental evaluation and is considered one of the efficient techniques for unary IND discovery. Still, there are drawback if the data set is too big to be sorted in main memory or if the number of simultaneously open files allowed by the OS system is reached \cite{papenbrock2015divide}. \\

In 2009 Bauckmann et al. also proposed a partialized version of $SPIDER$ in a section of the same paper. The authors found that there where surprisingly many partial inclusions (under a 5\% threshold) in their test data sets. To find partial inclusion dependencies, they first count how many distinct violations are present and in a second step consider the amount of duplicates for not included values. This means their algorithm does not immediately stop once a single validation has been found but only after an added counter surpasses a given threshold. The paper is not particular clear on how the number of duplicates is stored/retrieved and additionally does not analyse the computational effect of these changes and with the original source code being lost, this a approach can only be verified using a best guess approach.\\

The \textit{SAWFISH} algorithm \cite{kaminsky2023discovering}, published in 2023, is designed for identifying similarity inclusion dependencies (sINDs) within datasets, introducing a novel perspective on inclusion dependencies (INDs). While traditional INDs assume error-free data, \textit{SAWFISH} incorporates a similarity measure to accommodate minor errors like typos. Given a similarity threshold $\omega$ a sIND is valid if and only if for all tuples in the left hand side there exists a tuple in the right hand side which has at least a similarity of $\omega$ under a set similarity measure. The authors used the edit distance as well as the normalized edit distance. Through preprocessing, metadata generation, and a sliding-window approach, \textit{SAWFISH} successfully identifies and validates sIND candidates using an inverted index, providing a valuable tool for database applications despite dirty data challenges. \\

