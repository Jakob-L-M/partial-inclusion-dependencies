% This document should discuss passed approaches focusing on pros and cons

Inclusion dependencies (INDs) are a highly influential concept in both database research and practice, with a wide range of contributions and applications. The introduction already provided some insight into their diverse application areas of INDs. In this section, we will focus on the key achievements related to the implication problem of INDs. We will go over different algorithms and discuss their unique features. \\

In 1981 INDs started as a general notation of referential integrity, which was already a well established concept back then \cite{date1981referential}. Casanova et al. presented a paper \cite{casanova1982inclusion} on the inference rules of INDs. Three axioms where introduced: \textit{reflexivity}, \textit{transitivity} and \textit{projection and permutation}. The application of these rules to partial INDs will later be discussed in detail (Section \ref{theo:pInd}). The paper further proofed that the discovery of INDs is PSPACE-complete if there is no limit on the size of inclusions. Publications typically fall into three groups of algorithms. First, there are foreign key discovery algorithms. \\

In 1995 Bell and Brockhausen \cite{bell1995discovery} propose a graph-based approach to represent the relationships between attributes, allowing for a more efficient exploration of the search space. The algorithm is initiated with a directed graph, wherein all possible edges, which could not be pruned by statistical measures, are included. A directed edge in the graph represents an inclusion dependency, which is read as the edge from $A_i$ to $A_j$ ($A_i \rightarrow A_j$) represents the IND $A_i \subseteq A_j$. It then proceeds to remove those edges that failed the IND check. To determine the validity of an edge, the algorithm checks for transitivity, which enables it to answer whether a dependency could exist between two variables based on their relationships with a third variable, which was tested previously. If it is ascertained that a dependency is impossible, the algorithm skips the test and directly removes the given edge, thereby reducing the overall computational cost. The approach presented by Bell and Brockhausen for unary IND discovery has both reusable aspects and downsides. The algorithm's candidate generation technique, which uses data statistics such as data types and min-max values, can be reused in other discovery algorithms. This preprocessing step reduces the number of candidates that need to be validated and further reduce the storage overhead needed to store candidates. However, the validation technique used in the algorithm, which relies on SQL join-statements and requires accessing the data on disk, is infeasible for larger candidate sets. This limits the scalability of the approach and makes it less practical for large-scale data sets. Additionally, the need to store the data in a database and access it on disk for validation can add to the computational cost and time required for the discovery process.


The \textit{SAWFISH} algorithm \cite{kaminsky2023discovering} ...

In 2009 Bauckmann et al. proposed a partialized version of $SPIDER$ \cite{bauckmann2006efficiently}. The authors found that there where surprisingly many partial inclusions (under a 5\% threshold) in their test data sets.
- missing details
- no definitions in paper, only pseudocode