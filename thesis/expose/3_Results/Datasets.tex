\chapter{Datasets}
To understand the performance of the proposed algorithms it is crucial to perform testing on a variety of data sets. For this purpose we will gather some real word data sets. Further we will create synthetic data sets that aim on edge cases to see if the performance is strongly dependent on structural assumptions.

\section{Real World Data Sets}
There are many sources for csv or tsv files online. I have decided to gather data from the US Government\footnote{\href{https://data.gov}{data.gov}}, the European Union \footnote{\href{https://data.europe.eu}{data.europe.eu}}, Kaggle, \footnote{\href{https://kaggle.com}{Kaggle.com}}, Musicbrainz \footnote{\href{https://musicbrainz.org/}{musicbrainz.org}}, 

% TODO: MORE

\section{Synthetic Data Sets}
To test certain edge cases of the proposed algorithms, we will construct various edge case data sets. The \textit{SameSame} dataset consists of 32 attributes and 250.000 records. Each attribute carries the numbers 1 to 250.000 in the natural order. This means every attribute is a (partial) inclusion dependency of every other attribute. The same obviously also holds for combinations of columns. We will now calculate the expected number if (p)INDs in each layer. Since all candidates are perfect matches, the chosen threshold $\rho$ will not influence the number of pINDs. For u-nary INDs we find
$\frac{32 \cdot (32 + 1)}{2} = 528$ candidates and therefore also 528 INDs. The remaining layers will be calculated using the candidate formula previously defined (Section % TODO add ref
). Table % TODO add ref
 the number of candidate for the \textit{SameSame} dateset.
% TODO calculate INDs

